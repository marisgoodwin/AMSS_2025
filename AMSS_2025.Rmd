---
title: "AMSS_2025"
author: "Maris Goodwin"
date: "2024-12-29"
output: html_document
---
This is the analysis for my AMMS 2025 presentation.

The abstract is here: 

Title: Understanding seasonality of fish biodiversity using eDNA metabarcoding in Kachemak
Bay, Alaska
Authors: Maris R. Goodwin, Zack Gold, Jennifer L. Tusten, Jessica R. Glass
As global climate change rapidly impacts Arctic and subarctic marine ecosystems,
scientists need tools that will rigorously quantify changes in biological communities. Arctic
biodiversity monitoring is essential as glacial estuaries face recession, where the dynamic
convergence of freshwater and marine environments has historically supported diverse
ecological communities. Genomic sequencing tools such as environmental DNA (eDNA)
metabarcoding complement conventional fisheries approaches (e.g., netting) by providing in-
depth, rapid, and non-invasive assessments of species composition. We implemented eDNA
metabarcoding to assess its utility as an ecological monitoring tool in 5 estuarine sites in
Kachemak Bay, Alaska with varying degrees of glacial coverage (0 â€“ 60%). We analyzed
(n=100) eDNA samples from April and September 2022 in Kachemak Bay to tease apart
seasonal and spatial patterns in fish diversity and to assess the extent to which environmental
variables drive community composition. Results suggest a stronger influence of seasonal drivers
on fish biodiversity than spatial drivers. Detections of fish taxa were higher using eDNA than
conventional beach seining. The results of this study will build on existing baseline data to
document changes in a region heavily impacted by climate change.

A few things to note: 

All (seine and eDNA) data will be standardized from 0 to 1 to provide a semiquantitative value for comparison. In this transformation, the abundance values are first standardized by species maximum standardization, and then by sample total standardization, and by convention multiplied by 100 (here we will have values from 0 to 1). Essentially, each element is divided by its column maximum and then divided by the row total. Bray and Curtis (1957) employed a double standardization before ordination. In their study, tree species were measured on different scales than were shrubs and herbs (density and basal area for trees, frequency for herbs and shrubs), so that a species maximum standardization achieved a common scale. Their rationalization for the subsequent sample total standardization was that not all samples had the same number of measurements, and that the stand total standardizations achieved a more uniform basis for comparison. The environmental variables will be standardized as follows: 
salinity-
temperature- 
DO- 
turbidity-


# First we will load the necessary libraries for this analysis.
```{r load libs}
# Install and load necessary packages
# if (!requireNamespace("vegan", quietly = TRUE)) install.packages("vegan")
 if (!requireNamespace("ecodist", quietly = TRUE)) install.packages("ecodist")
# if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
# if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")

library(vegan)
library(ecodist)
library(ggplot2)
library(dplyr)
library(qiime2R)
library(phyloseq)
```

# Next we will load the data and take a look at it. 
```{r load data}
# Load the seine data including fish community data and taxa classification
seine <- read.csv("/Users/marisgoodwin/Documents/GITHUB/AMSS_2025/inputs/seine/2019_2020_2021_2022_Fish_Community_Data.csv")
seine_taxa <- read.csv("/Users/marisgoodwin/Documents/GITHUB/AMSS_2025/inputs/seine/fish_taxa_classification.csv")

# load in environmental data, this is also seine metadata
env <- read.csv("/Users/marisgoodwin/Documents/GITHUB/AMSS_2025/inputs/environmental/JNKB_20212022_Point_Sampling.csv")

# Load in the data from the L20230323_0629C Azenta run. This has samples from April and September 2022 in Kachemak Bay. 
edna_groups <- read.csv("/Users/marisgoodwin/Documents/GITHUB/AMSS_2025/inputs/eDNA/taxon_groups.csv")
edna_tax <- read.csv("/Users/marisgoodwin/Documents/GITHUB/AMSS_2025/inputs/eDNA/taxonomy_collapsed.csv")
method = "dada2-pe"
filtering = "unfiltered"
edna_table <- read_qza("/Users/marisgoodwin/Documents/GITHUB/AMSS_2025/inputs/eDNA/table.qza",method,filtering)

edna_meta <- read.csv("/Users/marisgoodwin/Documents/GITHUB/AMSS_2025/inputs/eDNA/Samples_kb_jn_samples.csv")
```
```{r reformatting}
# add in some reformatting of the data
edna_meta <- edna_meta %>%
  mutate(
    SamplingDate = mdy(SamplingDate),
    SamplingDate = format(SamplingDate, format = "%m/%d/%Y"),
    sample.name.library.prep = if_else(str_detect(sample.name.library.prep, "NC$"),
                                        str_replace(sample.name.library.prep, "NC$", "nc"),
                                        sample.name.library.prep)
  )
# Clean invalid characters and then apply the trimming and replacing operations
edna_meta <- edna_meta %>%
  mutate(Region = iconv(Region, from = "UTF-8", to = "UTF-8", sub = ""),  # Remove invalid characters
         Region = str_trim(Region),         # Remove trailing spaces
         Region = str_replace_all(Region, "Kachemak Bay", "Kachemak Bay"))  # 

month_mapping <- c(
  "Jan" = "January", "February" = "February", "Feb" = "February",
  "Mar" = "March", "March" = "March", "Apr" = "April", "April" = "April",
  "May" = "May", "Jun" = "June", "June" = "June", "Jul" = "July",
  "July" = "July", "Aug" = "August", "August" = "August",
  "Sep" = "September", "September" = "September"
)

# Clean and standardize the SamplingMonth column
edna_meta <- edna_meta %>%
  mutate(SamplingMonth = str_trim(SamplingMonth),  # Remove leading/trailing spaces
         SamplingMonth = dplyr::recode(SamplingMonth, !!!month_mapping))  # Recode to consistent names

env <- env %>%
  mutate(SamplingDate = mdy(SamplingDate),
         SamplingDate = format(SamplingDate, format = "%m/%d/%Y"))
seine <- seine %>%
  mutate(SamplingDate = mdy(SamplingDate),
         SamplingDate = format(SamplingDate, format = "%m/%d/%Y"))
```

# Now we are going to get only the data we are interested in for this analysis which is the fish community data and the environmental data from Kachemak Bay in 2022 April and September. 
```{r kb data}
# subset the edna meta by the samples from Kachemak Bay in 2022 April and September
kb_edna_meta <- edna_meta %>%
  filter(SamplingYear.x == 2022, SamplingMonth %in% c("April", "September"), Region == "Kachemak Bay")

# now do the same for the seine data
kb_seine <- seine %>%
  filter(SamplingYear == 2022, SamplingPeriod %in% c("1", "6"), Region == "kb")
# now for the environmental data
kb_env <- env %>%
  filter(SamplingYear == 2022, SamplingMonth %in% c("April", "September"), region == "kb")
```

# Now we are going to standardize the seine data. 
```{r standardize}
# first let's create a site by species matrix from the kb_seine data, the count is in the column "Count", the species is in the column "ScientificName," and we will make a unique identifier for each sample in the column "SampleID" this will combine the SiteAcronym and the SamplingPeriod

# create the unique identifier
kb_seine$SampleID <- paste(kb_seine$SiteAcronym, kb_seine$SamplingPeriod, sep = "_")

# first we need to aggregate 
kb_seine <- kb_seine %>%
  group_by(SampleID, ScientificName) %>%
  summarise(Count = sum(Count)) %>% 
  ungroup()
# create the site by species matrix
site_by_species <- kb_seine %>%
  dplyr::select(SampleID, ScientificName, Count) %>%
  spread(key = ScientificName, value = Count, fill = 0)
# now make the rownames the SampleID and remove the column 
rownames(site_by_species) <- site_by_species$SampleID
site_by_species <- site_by_species[, -1]
# now we want to standardize the data with a wisconsin double standardization
wisconsin <- wisconsin(site_by_species) 

# this now gives us a standardized site by species matrix with the proportion of each species in each sample from 0 to 1 

# now we want to do the same for the environmental data, let's aggregate by site acronym and sampling date, we will take the mean of the environmental variables, if there are multiple samples for a site and date 
kb_env <- kb_env %>%
  group_by(site_id,SamplingMonth) %>%
  summarise(Salinity = mean(Salinity, na.rm = TRUE),
            Temperature = mean(Temperature, na.rm = TRUE),
            DissolvedOxygen = mean(DissolvedOxygen, na.rm = TRUE),
            #for turbidity we have Turbidity_rep1 and Turbidity_rep2 and Turbidity_rep3 we will take the mean of these
            Turbidity = mean(c(Turbidity_rep1, Turbidity_rep2, Turbidity_rep3), na.rm = TRUE),
  # add in error for each variable 
  Salinity_error = sd(Salinity, na.rm = TRUE),
         Temperature_error = sd(Temperature, na.rm = TRUE),
         DissolvedOxygen_error = sd(DissolvedOxygen, na.rm = TRUE),
         Turbidity_error = sd(c(Turbidity_rep1, Turbidity_rep2, Turbidity_rep3), na.rm = TRUE)) %>%
  ungroup()
# not getting error for anything except turbidity, will look at this later 
```

```{r}
# let's do bray curtis dissimilarity and plot the results
# first we need to make a distance matrix
dist <- vegdist(wisconsin, method = "bray")

# now we can do a non-metric multidimensional scaling
nmds <- metaMDS(dist, k = 2, distance = "bray")

nmds_coords <- as.data.frame(nmds$points)
nmds_coords$SampleID <- site_by_species$SampleID # I had to do some fenagling for this, it will not run on its own until I fix the code above from deleting the sampleID column/rownames of the site_by species matrix

# add in a variable called Sampling Month which identifies the SampleID with the SamplingMonth. for example if the SampleID is jn_1 then the SamplingMonth is April
nmds_coords$SamplingMonth <- ifelse(str_detect(nmds_coords$SampleID, "1"), "April", "September")
# and site is the site acronym that comes before the underscore in the SampleID
nmds_coords$site <- str_extract(nmds_coords$SampleID, "^[^_]+")
# now we can plot the samples with ggplot by Sample ID
ggplot(nmds_coords, aes(x = MDS1, y = MDS2, color = SamplingMonth, shape = site)) +
  geom_point() +
  theme_minimal() +
  labs(title = "NMDS of Fish Community Data from Kachemak Bay 2022",
       x = "NMDS1",
       y = "NMDS2",
       color = "Sampling Month",
       shape = "Site")
ggsave("/Users/marisgoodwin/Documents/GITHUB/AMSS_2025/outputs/seine_NMDS_plot.png")

# Euclidean distance matrix
euclidean_dist <- dist(kb_env[,3:6], method = "euclidean")
# Mantel test
seine_mantel_test <- ecodist::mantel(dist ~ euclidean_dist)
print(seine_mantel_test)

# Correlation Coefficient (mantelr): The very low value (0.0083) indicates a very weak relationship between the biological and environmental distance matrices. P-values (pval1, pval2, pval3): All p-values are high (greater than 0.05), indicating that the observed correlation is not statistically significant. This means that there is no strong evidence to suggest a meaningful relationship between the two matrices.Confidence Interval (llim.2.5%, ulim.97.5%): The confidence interval includes zero, reinforcing the conclusion that the correlation is not significant. the Mantel test results suggest that there is no significant correlation between the biological community structure and the environmental variables in your data.

# PERMANOVA
permanova_test <- adonis2(wisconsin ~ SamplingMonth + site_id, data = kb_env, method = "bray")
print(permanova_test)


# okay, so I have averaged by site id and sampling month which is making it harder to interpret relationships between environmental variables and biological data. We , we do have multiple seines from a given site, so this is a way that we can add in a random effect to the model to account for this. this would  include a random effect for site in the  analysis to account for the nested structure of the data.
permanova_test <- adonis2(wisconsin ~ SamplingMonth + Temperature + Salinity + Turbidity + DissolvedOxygen + (1|site_id), data = kb_env, method = "bray")

```



